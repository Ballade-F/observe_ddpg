import numpy as np
import torch
import json
import os
import logging
import time
from datetime import datetime
import matplotlib.pyplot as plt

from mappo import MAPPOAgent
from sim_env import SimEnv

def setup_evaluation_logging():
    """è®¾ç½®è¯„ä¼°æ—¥å¿—è®°å½•"""
    timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M")
    log_dir = f"log/mappo_eval_{timestamp}"
    
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    log_filename = f"{log_dir}/evaluation.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__), log_dir

def get_global_state(env):
    """æ„å»ºå…¨å±€çŠ¶æ€"""
    agent_states = env.agentState.flatten()
    goal_states = env.goalState.flatten()
    obstacle_states = env.obstacles.flatten()
    
    global_state = np.concatenate([agent_states, goal_states, obstacle_states])
    return global_state

def evaluate_single_episode(agent: MAPPOAgent, config: dict, max_steps: int, 
                           render: bool = False, save_path: str = None) -> dict:
    """
    è¯„ä¼°å•ä¸ªepisode
    Args:
        agent: MAPPOæ™ºèƒ½ä½“
        config: ç¯å¢ƒé…ç½®
        max_steps: æœ€å¤§æ­¥æ•°
        render: æ˜¯å¦å¯è§†åŒ–
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¦‚æœéœ€è¦ä¿å­˜è½¨è¿¹å›¾ï¼‰
    Returns:
        episodeç»Ÿè®¡ä¿¡æ¯
    """
    env = SimEnv(config)
    env.reset()
    
    episode_reward = 0
    episode_step = 0
    success = False
    
    # è®°å½•è½¨è¿¹ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    if render or save_path:
        trajectory = {
            'agent_positions': [],
            'goal_states': [],
            'rewards': []
        }
    
    # è·å–åˆå§‹çŠ¶æ€
    current_global_state = get_global_state(env)
    current_agent_states = env.agentState
    current_observe_lengths = env.observeStateL
    current_observe_types = env.observeStateType
    
    for step in range(max_steps):
        # è®°å½•è½¨è¿¹
        if render or save_path:
            trajectory['agent_positions'].append(env.agentState.copy())
            trajectory['goal_states'].append(env.goalState.copy())
        
        # é€‰æ‹©åŠ¨ä½œï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰
        actions, _ = agent.choose_action(
            current_agent_states,
            current_observe_lengths,
            current_observe_types,
            deterministic=True
        )
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_agent_state, next_goal_state, next_obstacles, next_observe_lengths, next_observe_types, reward, done = env.step(actions)
        
        # è®°å½•å¥–åŠ±
        if render or save_path:
            trajectory['rewards'].append(reward)
        
        # æ›´æ–°çŠ¶æ€
        current_global_state = get_global_state(env)
        current_agent_states = next_agent_state
        current_observe_lengths = next_observe_lengths
        current_observe_types = next_observe_types
        
        episode_reward += reward
        episode_step += 1
        
        if done:
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if np.all(env.goalState[:, 2] > 0.5):
                success = True
            break
    
    # ç»Ÿè®¡å®Œæˆçš„ä»»åŠ¡ç‚¹ä¸ªæ•°
    num_completed_goals = np.sum(env.goalState[:, 2] > 0.5)
    
    # ä¿å­˜è½¨è¿¹å›¾
    if save_path and (render or save_path):
        plot_trajectory(env, trajectory, save_path)
    
    return {
        'reward': episode_reward,
        'steps': episode_step,
        'success': success,
        'completed_goals': num_completed_goals,
        'trajectory': trajectory if (render or save_path) else None
    }

def plot_trajectory(env, trajectory, save_path: str):
    """
    ç»˜åˆ¶æ™ºèƒ½ä½“è½¨è¿¹å›¾
    Args:
        env: ç¯å¢ƒå®ä¾‹
        trajectory: è½¨è¿¹æ•°æ®
        save_path: ä¿å­˜è·¯å¾„
    """
    try:
        fig, ax = plt.subplots(1, 1, figsize=(12, 10))
        
        # ç»˜åˆ¶ç¯å¢ƒè¾¹ç•Œ
        ax.set_xlim(env.kMinX - 0.5, env.kMaxX + 0.5)
        ax.set_ylim(env.kMinY - 0.5, env.kMaxY + 0.5)
        
        # ç»˜åˆ¶éšœç¢ç‰©
        for obs in env.obstacles:
            circle = plt.Circle((obs[0], obs[1]), obs[2], color='gray', alpha=0.7, label='éšœç¢ç‰©' if obs is env.obstacles[0] else "")
            ax.add_patch(circle)
        
        # ç»˜åˆ¶ç›®æ ‡ç‚¹
        final_goals = trajectory['goal_states'][-1]
        for i, goal in enumerate(final_goals):
            color = 'green' if goal[2] > 0.5 else 'red'
            marker = 'o' if goal[2] > 0.5 else 'x'
            ax.scatter(goal[0], goal[1], c=color, s=100, marker=marker, 
                      label='å·²å®Œæˆç›®æ ‡' if i == 0 and goal[2] > 0.5 else ('æœªå®Œæˆç›®æ ‡' if i == 0 and goal[2] <= 0.5 else ""))
        
        # ç»˜åˆ¶æ™ºèƒ½ä½“è½¨è¿¹
        colors = ['blue', 'orange', 'purple', 'brown', 'pink']
        for agent_id in range(env.kNumAgents):
            positions = np.array([pos[agent_id] for pos in trajectory['agent_positions']])
            ax.plot(positions[:, 0], positions[:, 1], 
                   color=colors[agent_id % len(colors)], linewidth=2, alpha=0.8,
                   label=f'æ™ºèƒ½ä½“{agent_id+1}è½¨è¿¹')
            
            # æ ‡è®°èµ·å§‹å’Œç»“æŸä½ç½®
            ax.scatter(positions[0, 0], positions[0, 1], 
                      color=colors[agent_id % len(colors)], s=150, marker='s', alpha=0.8)
            ax.scatter(positions[-1, 0], positions[-1, 1], 
                      color=colors[agent_id % len(colors)], s=150, marker='^', alpha=0.8)
        
        ax.set_xlabel('Xåæ ‡')
        ax.set_ylabel('Yåæ ‡')
        ax.set_title('MAPPOæ™ºèƒ½ä½“è½¨è¿¹å›¾')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"ç»˜åˆ¶è½¨è¿¹å›¾å¤±è´¥: {str(e)}")

def evaluate():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    # åŠ è½½è¯„ä¼°é…ç½®
    eval_config_path = './evaluate_config.json'
    if os.path.exists(eval_config_path):
        eval_config = json.load(open(eval_config_path, 'r'))
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        eval_config = {
            "model_path": "./log/mappo_train_latest/best_model",
            "num_episodes": 100,
            "render": False,
            "save_trajectories": True,
            "max_trajectory_saves": 10
        }
        # ä¿å­˜é»˜è®¤é…ç½®
        with open(eval_config_path, 'w', encoding='utf-8') as f:
            json.dump(eval_config, f, indent=2, ensure_ascii=False)
        print(f"å·²åˆ›å»ºé»˜è®¤è¯„ä¼°é…ç½®æ–‡ä»¶: {eval_config_path}")
    
    # åŠ è½½ç¯å¢ƒé…ç½®
    config_path = './config.json'
    config = json.load(open(config_path, 'r'))
    
    # è®¾ç½®æ—¥å¿—
    logger, log_dir = setup_evaluation_logging()
    
    logger.info("å¼€å§‹è¯„ä¼° MAPPO...")
    logger.info(f"è¯„ä¼°æ—¥å¿—ç›®å½•: {log_dir}")
    logger.info(f"æ¨¡å‹è·¯å¾„: {eval_config['model_path']}")
    logger.info(f"è¯„ä¼°è½®æ•°: {eval_config['num_episodes']}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç¯å¢ƒï¼ˆç”¨äºè·å–ç»´åº¦ä¿¡æ¯ï¼‰
    env = SimEnv(config)
    
    # è®¡ç®—çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
    agent_state_dim = 3
    observe_dim = env.kNumRadars
    action_dim = 2
    all_state_dim = env.kNumAgents * 3 + env.kNumGoals * 3 + env.kNumObstacles * 3
    max_action = np.array([env.kMaxSpeed, env.kMaxAngularSpeed])
    
    # åˆ›å»ºMAPPOæ™ºèƒ½ä½“
    agent = MAPPOAgent(
        agent_state_dim=agent_state_dim,
        observe_dim=observe_dim,
        all_state_dim=all_state_dim,
        action_dim=action_dim,
        num_agents=env.kNumAgents,
        max_action=max_action,
        device=device
    )
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    try:
        agent.load_models(eval_config['model_path'])
        logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return
    
    # è¯„ä¼°å‚æ•°
    num_episodes = eval_config['num_episodes']
    max_steps = config["training"]["max_steps_per_episode"]
    render = eval_config.get('render', False)
    save_trajectories = eval_config.get('save_trajectories', False)
    max_trajectory_saves = eval_config.get('max_trajectory_saves', 10)
    
    # åˆ›å»ºè½¨è¿¹ä¿å­˜ç›®å½•
    if save_trajectories:
        trajectory_dir = f"{log_dir}/trajectories"
        if not os.path.exists(trajectory_dir):
            os.makedirs(trajectory_dir)
    
    # è¯„ä¼°ç»Ÿè®¡
    all_rewards = []
    all_steps = []
    success_count = 0
    all_completed_goals = []
    
    logger.info("å¼€å§‹è¯„ä¼°å¾ªç¯...")
    start_time = time.time()
    
    for episode in range(num_episodes):
        episode_start_time = time.time()
        
        # å†³å®šæ˜¯å¦ä¿å­˜è½¨è¿¹
        save_path = None
        if save_trajectories and episode < max_trajectory_saves:
            save_path = f"{trajectory_dir}/episode_{episode+1}.png"
        
        # è¿è¡Œå•ä¸ªepisode
        result = evaluate_single_episode(
            agent, config, max_steps, render, save_path
        )
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        all_rewards.append(result['reward'])
        all_steps.append(result['steps'])
        all_completed_goals.append(result['completed_goals'])
        if result['success']:
            success_count += 1
        
        episode_time = time.time() - episode_start_time
        
        # æ¯10è½®è¾“å‡ºä¸€æ¬¡è¿›åº¦
        if (episode + 1) % 10 == 0:
            current_success_rate = success_count / (episode + 1)
            current_avg_reward = np.mean(all_rewards)
            current_avg_completed = np.mean(all_completed_goals)
            
            logger.info(f"Episode {episode+1}/{num_episodes} - "
                       f"æˆåŠŸç‡: {current_success_rate:.3f}, "
                       f"å¹³å‡å¥–åŠ±: {current_avg_reward:.3f}, "
                       f"å¹³å‡å®Œæˆä»»åŠ¡ç‚¹: {current_avg_completed:.2f}, "
                       f"ç”¨æ—¶: {episode_time:.2f}s")
    
    total_time = time.time() - start_time
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    final_success_rate = success_count / num_episodes
    final_avg_reward = np.mean(all_rewards)
    final_std_reward = np.std(all_rewards)
    final_avg_steps = np.mean(all_steps)
    final_avg_completed_goals = np.mean(all_completed_goals)
    final_std_completed_goals = np.std(all_completed_goals)
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    logger.info("=" * 60)
    logger.info("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    logger.info("=" * 60)
    logger.info(f"æ€»è¯„ä¼°è½®æ•°: {num_episodes}")
    logger.info(f"æ€»ç”¨æ—¶: {total_time:.2f}s")
    logger.info(f"å¹³å‡æ¯è½®ç”¨æ—¶: {total_time/num_episodes:.2f}s")
    logger.info("")
    logger.info("=== æ€§èƒ½ç»Ÿè®¡ ===")
    logger.info(f"æˆåŠŸç‡: {final_success_rate:.3f} ({success_count}/{num_episodes})")
    logger.info(f"å¹³å‡å¥–åŠ±: {final_avg_reward:.3f} Â± {final_std_reward:.3f}")
    logger.info(f"å¹³å‡æ­¥æ•°: {final_avg_steps:.1f}")
    logger.info(f"å¹³å‡å®Œæˆä»»åŠ¡ç‚¹: {final_avg_completed_goals:.2f} Â± {final_std_completed_goals:.2f}")
    logger.info(f"æœ€é«˜å¥–åŠ±: {np.max(all_rewards):.3f}")
    logger.info(f"æœ€ä½å¥–åŠ±: {np.min(all_rewards):.3f}")
    logger.info(f"æœ€å¤šå®Œæˆä»»åŠ¡ç‚¹: {int(np.max(all_completed_goals))}")
    logger.info(f"æœ€å°‘å®Œæˆä»»åŠ¡ç‚¹: {int(np.min(all_completed_goals))}")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    results = {
        'success_rate': final_success_rate,
        'avg_reward': final_avg_reward,
        'std_reward': final_std_reward,
        'avg_steps': final_avg_steps,
        'avg_completed_goals': final_avg_completed_goals,
        'std_completed_goals': final_std_completed_goals,
        'max_reward': float(np.max(all_rewards)),
        'min_reward': float(np.min(all_rewards)),
        'max_completed_goals': int(np.max(all_completed_goals)),
        'min_completed_goals': int(np.min(all_completed_goals)),
        'total_episodes': num_episodes,
        'total_time': total_time,
        'model_path': eval_config['model_path']
    }
    
    results_path = f"{log_dir}/evaluation_results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    
    if save_trajectories:
        logger.info(f"å‰{max_trajectory_saves}è½®è½¨è¿¹å›¾å·²ä¿å­˜åˆ°: {trajectory_dir}")
    
    logger.info("è¯„ä¼°ç»“æŸï¼")

if __name__ == "__main__":
    evaluate()
