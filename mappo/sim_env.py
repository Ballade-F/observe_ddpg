import math
from typing import List, Optional, Tuple
from matplotlib import pyplot as plt
import numpy as np
import json
from enum import Enum

class RadarObserveType(Enum):
    UnfinishedGoal = 0
    Empty = 1
    FinishedGoal = 2
    Obstacle = 3
    Agent = 4
    Boundary = 5
    
    



class SimEnv:
    def __init__(self, config: dict = None):
        """
        åˆå§‹åŒ–2Då¹³é¢ç¯å¢ƒ
        Args:
            config: ç¯å¢ƒé…ç½®

        configå‚æ•°å­—æ®µï¼š
            åœ°å›¾ï¼š
                æ™ºèƒ½ä½“æ•°é‡ï¼Œç›®æ ‡æ•°é‡ï¼Œéšœç¢ç‰©æ•°é‡ï¼Œéšœç¢ç‰©åŠå¾„èŒƒå›´ï¼Œ
                ç¯å¢ƒxyæœ€å¤§æœ€å°ï¼Œæ—¶é—´æ­¥é•¿
            æ™ºèƒ½ä½“ï¼š
                ä½“ç§¯åŠå¾„ï¼Œé€Ÿåº¦èŒƒå›´ï¼Œè§’é€Ÿåº¦èŒƒå›´
                æ¢æµ‹é›·è¾¾æ¡æ•°ï¼Œé›·è¾¾æœ€è¿œè·ç¦»
            ç›®æ ‡ï¼š
                åˆ°è¾¾ç›®æ ‡åˆ¤æ–­è·ç¦»
            å¥–åŠ±ï¼š
                æ—¶é—´æƒ©ç½šç³»æ•°ï¼Œç¢°æ’æƒ©ç½šç³»æ•°ï¼Œåˆ°è¾¾ç›®æ ‡å¥–åŠ±ç³»æ•°ï¼Œæ¥è¿‘ç›®æ ‡å¥–åŠ±ç³»æ•°
            
        """
        self.config = config

        self.kMaxX = config.get("map", {}).get("kMaxX", 5.0)
        self.kMaxY = config.get("map", {}).get("kMaxY", 5.0)
        self.kMinX = config.get("map", {}).get("kMinX", -5.0)
        self.kMinY = config.get("map", {}).get("kMinY", -5.0)
        self.kMaxObsR = config.get("map", {}).get("kMaxObsR", 1.5)
        self.kMinObsR = config.get("map", {}).get("kMinObsR", 0.5)
        self.kNumAgents = config.get("map", {}).get("kNumAgents", 1)
        self.kNumGoals = config.get("map", {}).get("kNumGoals", 1)
        self.kNumObstacles = config.get("map", {}).get("kNumObstacles", 3)
        self.kTimeStep = config.get("map", {}).get("kTimeStep", 0.1)

        self.kAgentRadius = config.get("agent", {}).get("kAgentRadius", 0.2)
        self.kMaxSpeed = config.get("agent", {}).get("kMaxSpeed", 1.0)
        self.kMaxAngularSpeed = config.get("agent", {}).get("kMaxAngularSpeed", 1.0)
        self.kNumRadars = config.get("agent", {}).get("kNumRadars", 32)
        self.kMaxRadarDist = config.get("agent", {}).get("kMaxRadarDist", 3.0)
        
        self.kGoalThreshold = config.get("goal", {}).get("kGoalThreshold", 0.5)
        
        self.kTimeReward = config.get("reward", {}).get("kTimeReward", -0.01)
        self.kCollisionReward = config.get("reward", {}).get("kCollisionReward", -1.0)
        self.kGoalReward = config.get("reward", {}).get("kGoalReward", 1.0)
        self.kDistanceReward = config.get("reward", {}).get("kDistanceReward", 0.1)
        self.kAgentSeparationReward = config.get("reward", {}).get("kAgentSeparationReward", 0.01)
        self.kMinAgentDistance = config.get("reward", {}).get("kMinAgentDistance", 1.0)
        self.kMinGoalDistance = config.get("reward", {}).get("kMinGoalDistance", 0.1)
        
        
        #åœ°å›¾å…ƒç´ åˆå§‹åæ ‡
        self.agentStartState:Optional[np.ndarray] = None    #(n_agents, 3) x, y, theta
        self.goalStartState:Optional[np.ndarray] = None    #(n_goals, 3) x, y, finish_flag
        self.obstacles:Optional[np.ndarray] = None    #(n_obstacles, 3) x, y, r


        #è¿‡ç¨‹å˜é‡
        self.agentState:Optional[np.ndarray] = None    #(n_agents, 3) x, y, theta
        self.observeStateL:Optional[np.ndarray] = None    #(n_agents, n_radars) è·ç¦»
        self.observeStateType:Optional[np.ndarray] = None    #(n_agents, n_radars) ç±»å‹
        self.goalState:Optional[np.ndarray] = None    #(n_goals, 3) x, y, finish_flag
        self.collision_flag:Optional[List[bool]] = None    #(n_agents)

        #ç¨‹åºä¸­ç”¨åˆ°çš„è¾…åŠ©å˜é‡
        self.radar_angle_range = np.linspace(-np.pi, np.pi, self.kNumRadars, endpoint=False)

        #åˆå§‹åŒ–
        self.generate_map()
        self.reset()

    def step(self, action: np.ndarray)->Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, bool]:
        """
        æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ
        Args:
            action: åŠ¨ä½œ (n_agents, 2) speed, angular_speed
        Returns:
            next_agentState: ä¸‹ä¸€ä¸ªæ™ºèƒ½ä½“çŠ¶æ€ (n_agents, 3) x, y, theta
            next_goalState: ä¸‹ä¸€ä¸ªç›®æ ‡çŠ¶æ€ (n_goals, 3) x, y, finish_flag
            next_obstacles: ä¸‹ä¸€ä¸ªéšœç¢ç‰©çŠ¶æ€ (n_obstacles, 3) x, y, r
            next_observeStateL: ä¸‹ä¸€ä¸ªè§‚æµ‹è·ç¦» (n_agents, n_radars)
            next_observeStateType: ä¸‹ä¸€ä¸ªè§‚æµ‹ç±»å‹ (n_agents, n_radars)
            rewards: æ•´ä¸ªteamçš„å¥–åŠ± (1,)
            done: æ˜¯å¦ç»“æŸ
        """
        # è§£æåŠ¨ä½œ
        agent_speed = action[:, 0]
        agent_angular_speed = action[:, 1]
        agent_speed = np.clip(agent_speed, -self.kMaxSpeed, self.kMaxSpeed)
        agent_angular_speed = np.clip(agent_angular_speed, -self.kMaxAngularSpeed, self.kMaxAngularSpeed)
        
        # æ›´æ–°æ™ºèƒ½ä½“çŠ¶æ€
        self.agentState[:, 2] += agent_angular_speed * self.kTimeStep
        # å¯¹æ¯ä¸ªæ™ºèƒ½ä½“çš„è§’åº¦è¿›è¡Œå½’ä¸€åŒ–
        for i in range(self.kNumAgents):
            self.agentState[i, 2] = self.normalize_theta(self.agentState[i, 2])
        self.agentState[:, 0] += agent_speed * np.cos(self.agentState[:, 2]) * self.kTimeStep
        self.agentState[:, 1] += agent_speed * np.sin(self.agentState[:, 2]) * self.kTimeStep

        # æ£€æŸ¥æ˜¯å¦ç¢°æ’
        for i in range(self.kNumAgents):
            if self.is_collision(self.agentState[i, :2], self.kAgentRadius):
                self.collision_flag[i] = True
         
        #æ£€æµ‹åˆ°è¾¾ç›®æ ‡
        agent_reach_goal_num = np.zeros(self.kNumAgents)
        for i in range(self.kNumAgents):
            reach_goal_index = self.update_reach_goal(self.agentState[i, :2])
            agent_reach_goal_num[i] = len(reach_goal_index)

        #æ›´æ–°è§‚æµ‹çŠ¶æ€
        self.update_observe_state()

        #è®¡ç®—å¥–åŠ± - ä½¿ç”¨æ”¹è¿›çš„å¥–åŠ±å‡½æ•°
        rewards = self.calculate_improved_reward(agent_reach_goal_num)
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ ä»»æ„ç¢°æ’æˆ–æ‰€æœ‰ç›®æ ‡éƒ½åˆ°è¾¾
        done = np.any(self.collision_flag) or np.all(self.goalState[:, 2] > 0.5)
        
        return self.agentState, self.goalState, self.obstacles, self.observeStateL, self.observeStateType, rewards, done

    def generate_map(self):
        """ç”Ÿæˆåœ°å›¾"""
        self.agentStartState = np.zeros((self.kNumAgents, 3))
        self.goalStartState = np.zeros((self.kNumGoals, 3))
        self.obstacles = np.zeros((self.kNumObstacles, 3))

        #ç”Ÿæˆéšœç¢
        for i in range(self.kNumObstacles):
            x = np.random.uniform(self.kMinX, self.kMaxX)
            y = np.random.uniform(self.kMinY, self.kMaxY)
            r = np.random.uniform(self.kMinObsR, self.kMaxObsR)
            self.obstacles[i] = np.array([x, y, r])

        #éšæœºç”Ÿæˆèµ·ç‚¹å’Œç›®æ ‡ç‚¹, èµ·ç‚¹å’Œç›®æ ‡ç‚¹ä¸èƒ½é‡åˆ, ä¸”ä¸èƒ½åœ¨éšœç¢ç‰©å†…
        # ä¸ºæ¯ä¸ªagentç”Ÿæˆèµ·ç‚¹
        for i in range(self.kNumAgents):
            start_collision = True
            while start_collision:
                start_pos = np.random.uniform(self.kMinX, self.kMaxX, 2)
                start_collision = self.is_collision(start_pos, self.kAgentRadius)
            start_angle = np.random.uniform(-np.pi, np.pi)
            self.agentStartState[i] = np.array([start_pos[0], start_pos[1], start_angle])
        
        # ä¸ºæ¯ä¸ªç›®æ ‡ç”Ÿæˆä½ç½®
        for i in range(self.kNumGoals):
            goal_collision = True
            while goal_collision:
                goal_pos = np.random.uniform(self.kMinX, self.kMaxX, 2)
                goal_collision = self.is_collision(goal_pos, self.kAgentRadius)
            self.goalStartState[i] = np.array([goal_pos[0], goal_pos[1], 0])

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.agentState = self.agentStartState.copy()
        self.goalState = self.goalStartState.copy()
        self.collision_flag = [False] * self.kNumAgents
        self.update_observe_state()

    def update_observe_state(self):
        """
        æ›´æ–°è§‚æµ‹çŠ¶æ€
        """
        self.observeStateL = np.zeros((self.kNumAgents, self.kNumRadars))
        self.observeStateType = np.zeros((self.kNumAgents, self.kNumRadars), dtype=int)
        for i in range(self.kNumAgents):
            for j in range(self.kNumRadars):
                angle = self.radar_angle_range[j] + self.agentState[i, 2] #é›·è¾¾è§’åº¦
                l,t = self.get_intersection(self.agentState[i, :2], angle, self.kMaxRadarDist)
                self.observeStateL[i, j] = l
                self.observeStateType[i, j] = t

    def get_intersection(self, start_pos: np.ndarray, angle: float, max_dist: float)->Tuple[float, int]:
        """
        è·å–é›·è¾¾ä¸éšœç¢ç‰©çš„äº¤ç‚¹
        Args:
            start_pos: é›·è¾¾èµ·ç‚¹ (x, y)
            angle: é›·è¾¾è§’åº¦ (å¼§åº¦,-pi~pi)
            max_dist: é›·è¾¾æœ€å¤§è·ç¦»
        Returns:
            l: äº¤ç‚¹è·ç¦»(çœŸå®åæ ‡ç³»)
            t: äº¤ç‚¹ç±»å‹
        """
        x, y = start_pos
        x_end = x + max_dist * np.cos(angle)
        y_end = y + max_dist * np.sin(angle)

        # åˆå§‹åŒ–æœ€å°è·ç¦»å’Œç±»å‹
        min_dist = max_dist
        min_type = RadarObserveType.Empty.value

        # æ£€æŸ¥åœ°å›¾è¾¹ç•Œ
        # è®¡ç®—å°„çº¿ä¸è¾¹ç•Œçš„äº¤ç‚¹
        if abs(np.cos(angle)) > 1e-6:  # é¿å…é™¤ä»¥0
            # æ£€æŸ¥å·¦å³è¾¹ç•Œ
            for bound_x in [self.kMinX, self.kMaxX]:
                t = (bound_x - x) / np.cos(angle)
                if t > 0:
                    y_intersect = y + t * np.sin(angle)
                    if self.kMinY <= y_intersect <= self.kMaxY:
                        dist = t
                        if dist < min_dist:
                            min_dist = dist
                            min_type = RadarObserveType.Boundary.value

        if abs(np.sin(angle)) > 1e-6:  # é¿å…é™¤ä»¥0
            # æ£€æŸ¥ä¸Šä¸‹è¾¹ç•Œ
            for bound_y in [self.kMinY, self.kMaxY]:
                t = (bound_y - y) / np.sin(angle)
                if t > 0:
                    x_intersect = x + t * np.cos(angle)
                    if self.kMinX <= x_intersect <= self.kMaxX:
                        dist = t
                        if dist < min_dist:
                            min_dist = dist
                            min_type = RadarObserveType.Boundary.value

        # æ£€æŸ¥éšœç¢ç‰©
        for obs_x, obs_y, obs_r in self.obstacles:
            # è®¡ç®—å°„çº¿åˆ°åœ†å¿ƒçš„è·ç¦»
            dx = x - obs_x
            dy = y - obs_y
            # a = np.cos(angle)**2 + np.sin(angle)**2
            a = 1
            b = 2 * (dx * np.cos(angle) + dy * np.sin(angle))
            c = dx**2 + dy**2 - obs_r**2
            
            # æ±‚è§£äºŒæ¬¡æ–¹ç¨‹
            discriminant = b**2 - 4*a*c
            if discriminant >= 0:
                t1 = (-b + np.sqrt(discriminant)) / (2*a)
                t2 = (-b - np.sqrt(discriminant)) / (2*a)
                for t in [t1, t2]:
                    if t > 0 and t < min_dist:
                        min_dist = t
                        min_type = RadarObserveType.Obstacle.value

        # æ£€æŸ¥ç›®æ ‡ç‚¹
        for i in range(self.kNumGoals):
            goal_x, goal_y, _ = self.goalStartState[i]
            # è®¡ç®—å°„çº¿åˆ°ç›®æ ‡ç‚¹çš„è·ç¦»
            dx = x - goal_x
            dy = y - goal_y
            a = 1
            b = 2 * (dx * np.cos(angle) + dy * np.sin(angle))
            c = dx**2 + dy**2 - self.kGoalThreshold**2
            
            discriminant = b**2 - 4*a*c
            if discriminant >= 0:
                t1 = (-b + np.sqrt(discriminant)) / (2*a)
                t2 = (-b - np.sqrt(discriminant)) / (2*a)
                for t in [t1, t2]:
                    if t > 0 and t < min_dist:
                        min_dist = t
                        min_type = RadarObserveType.UnfinishedGoal.value
                        if self.goalState[i, 2] > 0.5:
                            min_type = RadarObserveType.FinishedGoal.value

        # æ£€æŸ¥å…¶ä»–æ™ºèƒ½ä½“
        for agent_x, agent_y, _ in self.agentState:
            if np.allclose([agent_x, agent_y], start_pos[:2]):  # è·³è¿‡è‡ªå·±
                continue
            # è®¡ç®—å°„çº¿åˆ°æ™ºèƒ½ä½“çš„è·ç¦»
            dx = x - agent_x
            dy = y - agent_y
            a = 1
            b = 2 * (dx * np.cos(angle) + dy * np.sin(angle))
            c = dx**2 + dy**2 - self.kAgentRadius**2
            
            discriminant = b**2 - 4*a*c
            if discriminant >= 0:
                t1 = (-b + np.sqrt(discriminant)) / (2*a)
                t2 = (-b - np.sqrt(discriminant)) / (2*a)
                for t in [t1, t2]:
                    if t > 0 and t < min_dist:
                        min_dist = t
                        min_type = RadarObserveType.Agent.value

        return min_dist, min_type

    def is_collision(self, pos: np.ndarray, safe_distance:float=0.0)->bool:
        """
        åˆ¤æ–­ç‚¹æ˜¯å¦åœ¨éšœç¢ç‰©å†…
        Args:
            pos: ç‚¹åæ ‡(x, y)ï¼ŒçœŸå®åæ ‡ç³»
            safe_distance: å®‰å…¨è·ç¦»ï¼Œç”¨äºåˆ¤æ–­ç‚¹æ˜¯å¦åœ¨éšœç¢ç‰©å†…
        Returns:
            bool: æ˜¯å¦åœ¨éšœç¢ç‰©å†…
        """
        #å‡ºåœ°å›¾èŒƒå›´
        if pos[0] < self.kMinX or pos[0] > self.kMaxX or pos[1] < self.kMinY or pos[1] > self.kMaxY:
            return True
        for obs_x, obs_y, obs_r in self.obstacles:
            if np.linalg.norm(pos - np.array([obs_x, obs_y])) < (obs_r + safe_distance):
                return True
        return False
    
    def update_reach_goal(self, pos: np.ndarray)->List[int]:
        """
        æ›´æ–°åˆ°è¾¾ç›®æ ‡çŠ¶æ€
        Args:
            pos: agentä½ç½® (x, y)
        Returns:
            List[int]: åˆ°è¾¾ç›®æ ‡çš„ç´¢å¼•
        """
        reach_goal_index = []
        for i in range(self.kNumGoals):
            if self.goalState[i, 2] < 0.5 and np.linalg.norm(pos - self.goalState[i, :2]) < self.kGoalThreshold:
                self.goalState[i, 2] = 1
                reach_goal_index.append(i)
        return reach_goal_index
    
    def normalize_theta(self, theta: float) -> float:
        """å°†è§’åº¦å½’ä¸€åŒ–åˆ°[-pi, pi]"""
        while theta >= np.pi:
            theta -= 2 * np.pi
        while theta < -np.pi:
            theta += 2 * np.pi
        return theta
    
    def get_goal_distance_reward(self, agent_pos: np.ndarray) -> float:
        """
        è®¡ç®—æ™ºèƒ½ä½“åˆ°æ‰€æœ‰æœªå®Œæˆç›®æ ‡çš„è·ç¦»å€’æ•°å’Œå¥–åŠ±
        Args:
            agent_pos: æ™ºèƒ½ä½“ä½ç½® (x, y)
        Returns:
            float: è·ç¦»å€’æ•°å’Œå¥–åŠ±
        """
        unfinished_goals = self.goalState[self.goalState[:, 2] < 0.5]  # æœªå®Œæˆçš„ç›®æ ‡
        if len(unfinished_goals) == 0:
            return 0.0  # æ‰€æœ‰ç›®æ ‡éƒ½å®Œæˆäº†
        
        distances = np.linalg.norm(unfinished_goals[:, :2] - agent_pos, axis=1)
        # é™åˆ¶æœ€å°è·ç¦»ä»¥é¿å…æ— ç©·å¤§å¥–åŠ±
        distances = np.maximum(distances, self.kMinGoalDistance)
        
        # è®¡ç®—è·ç¦»å€’æ•°å’Œ
        inverse_distances = 1.0 / distances
        return np.sum(inverse_distances)
    
    def get_agent_separation_reward(self) -> float:
        """
        è®¡ç®—æ™ºèƒ½ä½“åˆ†ç¦»å¥–åŠ±ï¼Œé¼“åŠ±æ™ºèƒ½ä½“ä¿æŒé€‚å½“è·ç¦»
        Returns:
            float: åˆ†ç¦»å¥–åŠ±
        """
        separation_reward = 0.0
        for i in range(self.kNumAgents):
            for j in range(i + 1, self.kNumAgents):
                dist = np.linalg.norm(self.agentState[i, :2] - self.agentState[j, :2])
                if dist < self.kMinAgentDistance:
                    # è·ç¦»è¿‡è¿‘ï¼Œç»™äºˆæƒ©ç½š
                    separation_reward -= self.kAgentSeparationReward * (self.kMinAgentDistance - dist)

        return separation_reward
    

    
    def calculate_improved_reward(self, agent_reach_goal_num: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—ç®€åŒ–çš„å¥–åŠ±å‡½æ•°
        Args:
            agent_reach_goal_num: æ¯ä¸ªæ™ºèƒ½ä½“åˆ°è¾¾çš„ç›®æ ‡æ•°é‡
        Returns:
            np.ndarray: æ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±ï¼Œshape=(kNumAgents,)
        """
        rewards = np.zeros(self.kNumAgents)
        
        # 1. åŸºç¡€å¥–åŠ±ï¼ˆæ—¶é—´æƒ©ç½šã€ç¢°æ’æƒ©ç½šã€ç›®æ ‡å®Œæˆå¥–åŠ±ï¼‰
        for i in range(self.kNumAgents):
            rewards[i] += self.kTimeReward  # æ—¶é—´æƒ©ç½š
            
            if self.collision_flag[i]:
                rewards[i] += self.kCollisionReward  # ç¢°æ’æƒ©ç½š
            elif agent_reach_goal_num[i] > 0:
                rewards[i] += self.kGoalReward * agent_reach_goal_num[i]  # ç›®æ ‡å®Œæˆå¥–åŠ±
        
        # 2. ç›®æ ‡è·ç¦»å¥–åŠ±ï¼šä½¿ç”¨è·ç¦»å€’æ•°å’Œ
        for i in range(self.kNumAgents):
            if not self.collision_flag[i]:  # åªå¯¹æœªç¢°æ’çš„æ™ºèƒ½ä½“è®¡ç®—
                distance_reward = self.get_goal_distance_reward(self.agentState[i, :2])
                rewards[i] += self.kDistanceReward * distance_reward
        
        # 3. æ™ºèƒ½ä½“åˆ†ç¦»å¥–åŠ±ï¼ˆå¯é€‰ï¼‰
        # separation_reward = self.get_agent_separation_reward()
        # rewards += separation_reward / self.kNumAgents  # å¹³å‡åˆ†é…ç»™æ‰€æœ‰æ™ºèƒ½ä½“
        
        return rewards
    
    def plot_environment(self, save_path: str = "environment.png", fig_size: Tuple[int, int] = (10, 10), dpi: int = 100):
        """
        ç»˜åˆ¶å½“å‰ç¯å¢ƒçŠ¶æ€å¹¶ä¿å­˜ä¸ºPNGæ–‡ä»¶
        Args:
            save_path: ä¿å­˜æ–‡ä»¶çš„è·¯å¾„
            fig_size: å›¾åƒå¤§å° (width, height)
            dpi: å›¾åƒåˆ†è¾¨ç‡
        """
        fig, ax = plt.subplots(figsize=fig_size, dpi=dpi)
        
        # è®¾ç½®åœ°å›¾è¾¹ç•Œ
        ax.set_xlim(self.kMinX - 0.5, self.kMaxX + 0.5)
        ax.set_ylim(self.kMinY - 0.5, self.kMaxY + 0.5)
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        ax.set_title('Multi-Agent Environment State')
        
        # ç»˜åˆ¶åœ°å›¾è¾¹ç•Œ
        boundary_rect = plt.Rectangle((self.kMinX, self.kMinY), 
                                    self.kMaxX - self.kMinX, 
                                    self.kMaxY - self.kMinY,
                                    fill=False, edgecolor='black', linewidth=2)
        ax.add_patch(boundary_rect)
        
        # ç»˜åˆ¶éšœç¢ç‰©ï¼ˆç°è‰²åœ†å½¢ï¼‰
        for obs_x, obs_y, obs_r in self.obstacles:
            obstacle_circle = plt.Circle((obs_x, obs_y), obs_r, 
                                       color='gray', alpha=0.7, label='Obstacle' if obs_x == self.obstacles[0, 0] else "")
            ax.add_patch(obstacle_circle)
        
        # ç»˜åˆ¶ç›®æ ‡ç‚¹
        for i, (goal_x, goal_y, finish_flag) in enumerate(self.goalState):
            if finish_flag > 0.5:  # å·²å®Œæˆçš„ç›®æ ‡
                goal_circle = plt.Circle((goal_x, goal_y), self.kGoalThreshold, 
                                       color='green', alpha=0.6, label='Completed Goal' if i == 0 and finish_flag > 0.5 else "")
            else:  # æœªå®Œæˆçš„ç›®æ ‡
                goal_circle = plt.Circle((goal_x, goal_y), self.kGoalThreshold, 
                                       color='red', alpha=0.6, label='Uncompleted Goal' if i == 0 and finish_flag <= 0.5 else "")
            ax.add_patch(goal_circle)
            
            # åœ¨ç›®æ ‡ä¸­å¿ƒæ·»åŠ æ ‡å·
            ax.text(goal_x, goal_y, f'G{i}', ha='center', va='center', 
                   fontsize=8, fontweight='bold', color='white')
        
        # ç»˜åˆ¶æ™ºèƒ½ä½“
        colors = ['blue', 'orange', 'purple', 'brown', 'pink', 'olive', 'cyan', 'magenta']
        for i, (agent_x, agent_y, agent_theta) in enumerate(self.agentState):
            color = colors[i % len(colors)]
            
            # ç»˜åˆ¶æ™ºèƒ½ä½“ä¸»ä½“ï¼ˆåœ†å½¢ï¼‰
            if self.collision_flag[i]:
                # ç¢°æ’çš„æ™ºèƒ½ä½“ç”¨çº¢è‰²è¾¹æ¡†
                agent_circle = plt.Circle((agent_x, agent_y), self.kAgentRadius, 
                                        color=color, alpha=0.8, edgecolor='red', linewidth=3,
                                        label=f'Agent {i} (Collision)' if i == 0 and self.collision_flag[i] else "")
            else:
                agent_circle = plt.Circle((agent_x, agent_y), self.kAgentRadius, 
                                        color=color, alpha=0.8,
                                        label=f'Agent {i}' if i == 0 and not self.collision_flag[i] else "")
            ax.add_patch(agent_circle)
            
            # ç»˜åˆ¶æ™ºèƒ½ä½“æœå‘ç®­å¤´
            arrow_length = self.kAgentRadius * 1.5
            dx = arrow_length * np.cos(agent_theta)
            dy = arrow_length * np.sin(agent_theta)
            ax.arrow(agent_x, agent_y, dx, dy, 
                    head_width=self.kAgentRadius*0.3, head_length=self.kAgentRadius*0.2, 
                    fc=color, ec=color, alpha=0.9)
            
            # åœ¨æ™ºèƒ½ä½“ä¸­å¿ƒæ·»åŠ æ ‡å·
            ax.text(agent_x, agent_y, f'A{i}', ha='center', va='center', 
                   fontsize=8, fontweight='bold', color='white')
        
        # ç»˜åˆ¶é›·è¾¾çº¿
        radar_colors = {
            RadarObserveType.Empty.value: 'lightgray',
            RadarObserveType.Obstacle.value: 'red', 
            RadarObserveType.FinishedGoal.value: 'lightgreen',
            RadarObserveType.UnfinishedGoal.value: 'orange',
            RadarObserveType.Agent.value: 'lightblue',
            RadarObserveType.Boundary.value: 'black'
        }
        
        radar_labels = {
            RadarObserveType.Empty.value: 'Radar Empty',
            RadarObserveType.Obstacle.value: 'Radar Obstacle', 
            RadarObserveType.FinishedGoal.value: 'Radar Finished Goal',
            RadarObserveType.UnfinishedGoal.value: 'Radar Unfinished Goal',
            RadarObserveType.Agent.value: 'Radar Agent',
            RadarObserveType.Boundary.value: 'Radar Boundary'
        }
        
        # è®°å½•å·²æ·»åŠ åˆ°å›¾ä¾‹çš„é›·è¾¾ç±»å‹
        radar_legend_added = set()
        
        for i, (agent_x, agent_y, agent_theta) in enumerate(self.agentState):
            for j in range(self.kNumRadars):
                # è®¡ç®—é›·è¾¾çº¿è§’åº¦
                radar_angle = self.radar_angle_range[j] + agent_theta
                
                # è·å–é›·è¾¾æ¢æµ‹è·ç¦»å’Œç±»å‹
                radar_dist = self.observeStateL[i, j]
                radar_type = self.observeStateType[i, j]
                
                # è®¡ç®—é›·è¾¾çº¿ç»ˆç‚¹åæ ‡
                end_x = agent_x + radar_dist * np.cos(radar_angle)
                end_y = agent_y + radar_dist * np.sin(radar_angle)
                
                # è·å–å¯¹åº”ç±»å‹çš„é¢œè‰²
                line_color = radar_colors.get(radar_type, 'gray')
                
                # ç¡®å®šæ˜¯å¦éœ€è¦æ·»åŠ å›¾ä¾‹æ ‡ç­¾
                label = None
                if radar_type not in radar_legend_added:
                    label = radar_labels.get(radar_type, f'Radar Type {radar_type}')
                    radar_legend_added.add(radar_type)
                
                # ç»˜åˆ¶é›·è¾¾çº¿
                ax.plot([agent_x, end_x], [agent_y, end_y], 
                       color=line_color, alpha=0.6, linewidth=1, label=label)
        
        # æ·»åŠ å›¾ä¾‹
        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), ncol=1)
        
        # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
        plt.tight_layout()
        plt.savefig(save_path, bbox_inches='tight', dpi=dpi)
        plt.close()
        
        print(f"Environment state plot saved to: {save_path}")
    
    
def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•SimEnvç±»å¹¶ç”Ÿæˆç¯å¢ƒçŠ¶æ€å›¾
    """
    # åˆ›å»ºç¯å¢ƒé…ç½®
    config = {
        "map": {
            "kMaxX": 5.0,
            "kMaxY": 5.0,
            "kMinX": -5.0,
            "kMinY": -5.0,
            "kMaxObsR": 1.0,
            "kMinObsR": 0.3,
            "kNumAgents": 3,
            "kNumGoals": 2,
            "kNumObstacles": 5,
            "kTimeStep": 0.1
        },
        "agent": {
            "kAgentRadius": 0.2,
            "kMaxSpeed": 1.0,
            "kMaxAngularSpeed": 1.0,
            "kNumRadars": 32,
            "kMaxRadarDist": 3.0
        },
        "goal": {
            "kGoalThreshold": 0.3
        },
        "reward": {
            "kTimeReward": -0.01,
            "kCollisionReward": -1.0,
            "kGoalReward": 1.0,
            "kDistReward": 0.01
        }
    }

    # è®¾ç½®numpyçš„éšæœºç§å­
    np.random.seed(1)
    
    # å®ä¾‹åŒ–ç¯å¢ƒ
    print("Creating environment...")
    env = SimEnv(config)
    
    # ç”Ÿæˆå¹¶ä¿å­˜åˆå§‹ç¯å¢ƒçŠ¶æ€å›¾
    print("Generating environment state plot...")
    env.plot_environment(save_path="sim_env_initial.png", fig_size=(12, 10), dpi=150)
    
    # æ‰§è¡Œå‡ æ­¥åŠ¨ä½œæ¥å±•ç¤ºåŠ¨æ€æ•ˆæœ
    print("Executing random actions...")
    for step in range(5):
        # ç”ŸæˆéšæœºåŠ¨ä½œ
        actions = np.random.uniform(-0.5, 0.5, (env.kNumAgents, 2))
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_agentState, next_goalState, next_obstacles, next_observeStateL, next_observeStateType, rewards, done = env.step(actions)
        
        print(f"Step {step+1}: Rewards = {rewards}, Done = {done}")
        
        if done:
            print("Environment finished!")
            break
    
    # ä¿å­˜æœ€ç»ˆçŠ¶æ€å›¾
    # ä¿®æ”¹goal1çš„çŠ¶æ€ä¸ºå·²å®Œæˆï¼Œæµ‹è¯•é›·è¾¾çº¿æ˜¯å¦æ­£ç¡®
    env.goalState[1, 2] = 1
    env.update_observe_state()
    env.plot_environment(save_path="sim_env_final.png", fig_size=(12, 10), dpi=150)
    print("Test completed! Generated initial state plot (sim_env_initial.png) and final state plot (sim_env_final.png)")


def test_reward_function():
    """
    æµ‹è¯•ç®€åŒ–çš„å¥–åŠ±å‡½æ•°
    """
    print("=" * 60)
    print("æµ‹è¯•ç®€åŒ–çš„å¥–åŠ±å‡½æ•°...")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = {
        "map": {
            "kMaxX": 5.0, "kMaxY": 5.0, "kMinX": -5.0, "kMinY": -5.0,
            "kMaxObsR": 1.0, "kMinObsR": 0.3, "kNumAgents": 2, "kNumGoals": 3, "kNumObstacles": 2,
            "kTimeStep": 0.1
        },
        "agent": {
            "kAgentRadius": 0.2, "kMaxSpeed": 1.0, "kMaxAngularSpeed": 1.0,
            "kNumRadars": 32, "kMaxRadarDist": 3.0
        },
        "goal": {"kGoalThreshold": 0.3},
        "reward": {
            "kTimeReward": -0.1, "kCollisionReward": -10.0, "kGoalReward": 5.0,
            "kDistanceReward": 0.01, "kAgentSeparationReward": 0.1,
            "kMinAgentDistance": 1.0, "kMinGoalDistance": 0.1
        }
    }
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    
    # åˆ›å»ºç¯å¢ƒ
    env = SimEnv(config)
    print(f"âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"  æ™ºèƒ½ä½“æ•°é‡: {env.kNumAgents}")
    print(f"  ç›®æ ‡æ•°é‡: {env.kNumGoals}")
    print(f"  éšœç¢ç‰©æ•°é‡: {env.kNumObstacles}")
    
    # æµ‹è¯•åœºæ™¯1ï¼šæ™ºèƒ½ä½“è¿œç¦»ç›®æ ‡
    print(f"\n{'='*30}")
    print("æµ‹è¯•åœºæ™¯1ï¼šæ™ºèƒ½ä½“è¿œç¦»ç›®æ ‡")
    print(f"{'='*30}")
    
    env.reset()
    # æ‰‹åŠ¨è®¾ç½®æ™ºèƒ½ä½“ä½ç½®ï¼ˆè¿œç¦»ç›®æ ‡ï¼‰
    env.agentState[0] = [-4, -4, 0]  # æ™ºèƒ½ä½“0åœ¨å·¦ä¸‹è§’
    env.agentState[1] = [4, 4, 0]    # æ™ºèƒ½ä½“1åœ¨å³ä¸Šè§’
    
    # è®¡ç®—å¥–åŠ±
    agent_reach_goal_num = np.zeros(env.kNumAgents)
    reward1 = env.calculate_improved_reward(agent_reach_goal_num)
    print(f"è¿œç¦»ç›®æ ‡æ—¶çš„å¥–åŠ±: {reward1:.4f}")
    
    # æµ‹è¯•åœºæ™¯2ï¼šæ™ºèƒ½ä½“é è¿‘ç›®æ ‡
    print(f"\n{'='*30}")
    print("æµ‹è¯•åœºæ™¯2ï¼šæ™ºèƒ½ä½“é è¿‘ç›®æ ‡")
    print(f"{'='*30}")
    
    # å°†æ™ºèƒ½ä½“ç§»åŠ¨åˆ°ç›®æ ‡é™„è¿‘
    if len(env.goalState) > 0:
        env.agentState[0, :2] = env.goalState[0, :2] + [0.5, 0]  # æ™ºèƒ½ä½“0é è¿‘ç›®æ ‡0
    if len(env.goalState) > 1:
        env.agentState[1, :2] = env.goalState[1, :2] + [0, 0.5]  # æ™ºèƒ½ä½“1é è¿‘ç›®æ ‡1
    
    reward2 = env.calculate_improved_reward(agent_reach_goal_num)
    print(f"é è¿‘ç›®æ ‡æ—¶çš„å¥–åŠ±: {reward2:.4f}")
    print(f"å¥–åŠ±æ”¹å–„: {reward2 - reward1:.4f}")
    
    # æµ‹è¯•åœºæ™¯3ï¼šæ™ºèƒ½ä½“è¿‡äºæ¥è¿‘
    print(f"\n{'='*30}")
    print("æµ‹è¯•åœºæ™¯3ï¼šæ™ºèƒ½ä½“è¿‡äºæ¥è¿‘")
    print(f"{'='*30}")
    
    # å°†ä¸¤ä¸ªæ™ºèƒ½ä½“æ”¾åœ¨å¾ˆè¿‘çš„ä½ç½®
    env.agentState[0, :2] = [0, 0]
    env.agentState[1, :2] = [0.2, 0]  # è·ç¦»0.2ï¼Œå°äºæœ€å°è·ç¦»1.0
    
    reward3 = env.calculate_improved_reward(agent_reach_goal_num)
    print(f"æ™ºèƒ½ä½“è¿‡è¿‘æ—¶çš„å¥–åŠ±: {reward3:.4f}")
    
    # æµ‹è¯•åœºæ™¯4ï¼šæ™ºèƒ½ä½“é€‚å½“åˆ†ç¦»
    print(f"\n{'='*30}")
    print("æµ‹è¯•åœºæ™¯4ï¼šæ™ºèƒ½ä½“é€‚å½“åˆ†ç¦»")
    print(f"{'='*30}")
    
    # å°†æ™ºèƒ½ä½“åˆ†å¼€é€‚å½“è·ç¦»
    env.agentState[0, :2] = [0, 0]
    env.agentState[1, :2] = [2, 0]  # è·ç¦»2.0ï¼Œå¤§äºæœ€å°è·ç¦»
    
    reward4 = env.calculate_improved_reward(agent_reach_goal_num)
    print(f"æ™ºèƒ½ä½“é€‚å½“åˆ†ç¦»æ—¶çš„å¥–åŠ±: {reward4:.4f}")
    print(f"ä¸è¿‡è¿‘æƒ…å†µç›¸æ¯”æ”¹å–„: {reward4 - reward3:.4f}")
    
    # æµ‹è¯•åœºæ™¯5ï¼šæ™ºèƒ½ä½“åˆ°è¾¾ç›®æ ‡
    print(f"\n{'='*30}")
    print("æµ‹è¯•åœºæ™¯5ï¼šæ™ºèƒ½ä½“åˆ°è¾¾ç›®æ ‡")
    print(f"{'='*30}")
    
    # æ¨¡æ‹Ÿæ™ºèƒ½ä½“åˆ°è¾¾ç›®æ ‡
    agent_reach_goal_num[0] = 1  # æ™ºèƒ½ä½“0åˆ°è¾¾1ä¸ªç›®æ ‡
    reward5 = env.calculate_improved_reward(agent_reach_goal_num)
    print(f"åˆ°è¾¾ç›®æ ‡æ—¶çš„å¥–åŠ±: {reward5:.4f}")
    print(f"ä¸æœªåˆ°è¾¾ç›¸æ¯”æ”¹å–„: {reward5 - reward4:.4f}")
    
    # æµ‹è¯•è·ç¦»å€’æ•°å’Œå¥–åŠ±çš„ç‰¹æ€§
    print(f"\n{'='*30}")
    print("æµ‹è¯•è·ç¦»å€’æ•°å’Œå¥–åŠ±ç‰¹æ€§")
    print(f"{'='*30}")
    
    env.reset()
    # æµ‹è¯•å•ä¸ªæ™ºèƒ½ä½“çš„è·ç¦»å¥–åŠ±
    agent_pos = np.array([0.0, 0.0])
    distance_reward = env.get_goal_distance_reward(agent_pos)
    print(f"æ™ºèƒ½ä½“åœ¨åŸç‚¹æ—¶çš„è·ç¦»å¥–åŠ±: {distance_reward:.4f}")
    
    # å°†æ™ºèƒ½ä½“ç§»åŠ¨åˆ°æ›´é è¿‘ç›®æ ‡çš„ä½ç½®
    if len(env.goalState) > 0:
        closer_pos = env.goalState[0, :2] * 0.8  # æ›´é è¿‘ç¬¬ä¸€ä¸ªç›®æ ‡
        closer_distance_reward = env.get_goal_distance_reward(closer_pos)
        print(f"æ™ºèƒ½ä½“é è¿‘ç›®æ ‡æ—¶çš„è·ç¦»å¥–åŠ±: {closer_distance_reward:.4f}")
        print(f"å¥–åŠ±æå‡: {closer_distance_reward - distance_reward:.4f}")
    
    print(f"\n{'='*60}")
    print("ğŸ‰ ç®€åŒ–å¥–åŠ±å‡½æ•°æµ‹è¯•å®Œæˆï¼")
    print("âœ“ è·ç¦»å€’æ•°å’Œå¥–åŠ±ï¼šæ™ºèƒ½ä½“è¶Šé è¿‘ç›®æ ‡å¥–åŠ±è¶Šé«˜")
    print("âœ“ åˆ†ç¦»å¥–åŠ±ï¼šæ™ºèƒ½ä½“ä¿æŒé€‚å½“è·ç¦»")
    print("âœ“ åŸºç¡€å¥–åŠ±ï¼šæ—¶é—´æƒ©ç½šã€ç¢°æ’æƒ©ç½šã€ç›®æ ‡å®Œæˆå¥–åŠ±")
    print("âœ“ é¿å…æ— ç©·å¤§ï¼šæœ€å°è·ç¦»é™åˆ¶ç¡®ä¿æ•°å€¼ç¨³å®š")
    print(f"{'='*60}")

if __name__ == "__main__":
    test_reward_function()
        
    
    