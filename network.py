import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

#TODO:ä»¥åéœ€è¦è€ƒè™‘å½’ä¸€åŒ–çš„é—®é¢˜
#TODO:æ”¹æˆattentionç½‘ç»œï¼Œå°¤å…¶æ˜¯criticã€‚æœ€é‡è¦è€ƒè™‘çš„æ˜¯è®©ç½‘ç»œä¸æ™ºèƒ½ä½“æ•°é‡ã€ï¼ˆé›·è¾¾æ¡æ•°ï¼‰æ— å…³
#TODO: è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªé©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“ä»¥å¾€çš„è§‚æµ‹åº”å½“å½±å“å½“å‰çš„å†³ç­–ï¼Œéœ€è¦è€ƒè™‘å†å²è§‚æµ‹ï¼Œå¯ä»¥è¯•è¯•SLAMä¸­çš„å…³é”®å¸§æ€æƒ³

class Actor(nn.Module):
    def __init__(self, self_state_dim: int, observe_state_dim: int, action_dim: int, 
                 hidden_dim: int, max_action: np.ndarray, device: torch.device):
        super(Actor, self).__init__()
        self.max_action = torch.FloatTensor(max_action)
        self.device = device

        self.l1 = nn.Linear(self_state_dim + 2*observe_state_dim, hidden_dim)
        self.l2 = nn.Linear(hidden_dim, hidden_dim)
        self.l3 = nn.Linear(hidden_dim, hidden_dim)
        self.l4 = nn.Linear(hidden_dim, action_dim)
        
        # Kaimingåˆå§‹åŒ–
        self._init_weights()
        
        self.to(device)
    
    def _init_weights(self):
        """Kaimingåˆå§‹åŒ–"""
        for layer in [self.l1, self.l2, self.l3]:
            nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
            nn.init.constant_(layer.bias, 0)
        # è¾“å‡ºå±‚ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–
        nn.init.uniform_(self.l4.weight, -3e-3, 3e-3)
        nn.init.constant_(self.l4.bias, 0)
    
    def forward(self, self_state: torch.Tensor, observe_length: torch.Tensor, observe_type: torch.Tensor) -> torch.Tensor:
        '''
        self_state: (batch_size, self_state_dim) æ™ºèƒ½ä½“è‡ªèº«çŠ¶æ€ x,y,theta
        observe_length: (batch_size, observe_state_dim) é›·è¾¾æ¢æµ‹åˆ°çš„éšœç¢ç‰©è·ç¦»
        observe_type: (batch_size, observe_state_dim) é›·è¾¾æ¢æµ‹åˆ°çš„éšœç¢ç‰©ç±»å‹
        '''
        self_state = self_state.to(self.device)
        observe_length = observe_length.to(self.device)
        observe_type = observe_type.to(self.device)
        x = torch.cat([self_state, observe_length, observe_type], dim=1)    
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        x = F.relu(self.l3(x))
        x = torch.mul(self.max_action.to(self.device), torch.tanh(self.l4(x)))
        return x

class Critic(nn.Module):
    def __init__(self, all_state_dim: int, all_action_dim: int, hidden_dim: int, device: torch.device):
        super(Critic, self).__init__()
        self.device = device

        # all_state_dim: å…¨å±€çŠ¶æ€ç»´åº¦ï¼ŒåŒ…å«æ‰€æœ‰agentçŠ¶æ€ã€ç›®æ ‡çŠ¶æ€ã€éšœç¢ç‰©çŠ¶æ€
        # all_state_dim = agent_state_dim*num_agents + goal_state_dim*num_goals + obstacles_state_dim*num_obstacles
        # å³ 3*num_agents + 3*num_goals + 3*num_obstacles
        self.l1 = nn.Linear(all_state_dim + all_action_dim, hidden_dim)
        self.l2 = nn.Linear(hidden_dim, hidden_dim)
        self.l3 = nn.Linear(hidden_dim, hidden_dim)
        self.l4 = nn.Linear(hidden_dim, 1)
        
        # Kaimingåˆå§‹åŒ–
        self._init_weights()
        
        self.to(device)
    
    def _init_weights(self):
        """Kaimingåˆå§‹åŒ–"""
        for layer in [self.l1, self.l2, self.l3]:
            nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
            nn.init.constant_(layer.bias, 0)
        # è¾“å‡ºå±‚ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–
        nn.init.uniform_(self.l4.weight, -3e-3, 3e-3)
        nn.init.constant_(self.l4.bias, 0)
    
    def forward(self, all_state: torch.Tensor, all_action: torch.Tensor) -> torch.Tensor:
        all_state = all_state.to(self.device)
        all_action = all_action.to(self.device)
        x = torch.cat([all_state, all_action], dim=1)
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        x = F.relu(self.l3(x))
        x = self.l4(x)
        return x
    
def test_network():
    """
    æµ‹è¯•Actorå’ŒCriticç½‘ç»œçš„åŠŸèƒ½å’Œç»´åº¦
    """
    print("=" * 60)
    print("å¼€å§‹æµ‹è¯•ç½‘ç»œæ¨¡å—...")
    print("=" * 60)
    
    # è®¾ç½®æµ‹è¯•å‚æ•° - åŸºäºconfig.jsonçš„é»˜è®¤é…ç½®
    num_agents = 2
    num_goals = 3
    num_obstacles = 5
    
    # ç½‘ç»œç»´åº¦å‚æ•°
    self_state_dim = 3  # x, y, theta
    observe_state_dim = 32  # é›·è¾¾æ¡æ•°
    action_dim = 2  # speed, angular_speed
    all_state_dim = num_agents * 3 + num_goals * 3 + num_obstacles * 3  # å…¨å±€çŠ¶æ€ç»´åº¦
    all_action_dim = action_dim * num_agents  # æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œç»´åº¦
    hidden_dim = 256
    max_action = np.array([1.0, 1.0])  # æœ€å¤§é€Ÿåº¦å’Œè§’é€Ÿåº¦
    batch_size = 5
    
    # è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    print(f"\næµ‹è¯•å‚æ•°:")
    print(f"  æ™ºèƒ½ä½“æ•°é‡: {num_agents}")
    print(f"  ç›®æ ‡æ•°é‡: {num_goals}")
    print(f"  éšœç¢ç‰©æ•°é‡: {num_obstacles}")
    print(f"  æ™ºèƒ½ä½“çŠ¶æ€ç»´åº¦: {self_state_dim}")
    print(f"  è§‚æµ‹ç»´åº¦: {observe_state_dim}")
    print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"  å…¨å±€çŠ¶æ€ç»´åº¦: {all_state_dim}")
    print(f"  å…¨å±€åŠ¨ä½œç»´åº¦: {all_action_dim}")
    print(f"  éšè—å±‚ç»´åº¦: {hidden_dim}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  æœ€å¤§åŠ¨ä½œ: {max_action}")
    
    try:
        # æµ‹è¯•Actorç½‘ç»œ
        print(f"\n{'='*30}")
        print("æµ‹è¯•Actorç½‘ç»œ...")
        print(f"{'='*30}")
        
        actor = Actor(self_state_dim, observe_state_dim, action_dim, hidden_dim, max_action, device)
        print(f"âœ“ Actorç½‘ç»œåˆ›å»ºæˆåŠŸ")
        print(f"  ç½‘ç»œå‚æ•°æ•°é‡: {sum(p.numel() for p in actor.parameters())}")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        self_state = torch.randn(batch_size, self_state_dim)
        observe_length = torch.randn(batch_size, observe_state_dim)
        observe_type = torch.randn(batch_size, observe_state_dim)
        
        print(f"\nè¾“å…¥ç»´åº¦:")
        print(f"  self_state: {self_state.shape}")
        print(f"  observe_length: {observe_length.shape}")
        print(f"  observe_type: {observe_type.shape}")
        
        # å‰å‘ä¼ æ’­
        action = actor(self_state, observe_length, observe_type)
        print(f"\nè¾“å‡ºç»´åº¦:")
        print(f"  action: {action.shape}")
        print(f"  æœŸæœ›ç»´åº¦: ({batch_size}, {action_dim})")
        
        
        # æ£€æŸ¥ç»´åº¦æ˜¯å¦æ­£ç¡®
        assert action.shape == (batch_size, action_dim), f"Actorè¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›{(batch_size, action_dim)}, å®é™…{action.shape}"
        
        # æ£€æŸ¥åŠ¨ä½œæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        assert torch.all(action >= -torch.FloatTensor(max_action).to(device) - 1e-6), "åŠ¨ä½œå€¼ä½äºæœ€å°å€¼"
        assert torch.all(action <= torch.FloatTensor(max_action).to(device) + 1e-6), "åŠ¨ä½œå€¼é«˜äºæœ€å¤§å€¼"
        
        print(f"âœ“ Actorç½‘ç»œæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Criticç½‘ç»œ
        print(f"\n{'='*30}")
        print("æµ‹è¯•Criticç½‘ç»œ...")
        print(f"{'='*30}")
        
        critic = Critic(all_state_dim, all_action_dim, hidden_dim, device)
        print(f"âœ“ Criticç½‘ç»œåˆ›å»ºæˆåŠŸ")
        print(f"  ç½‘ç»œå‚æ•°æ•°é‡: {sum(p.numel() for p in critic.parameters())}")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        all_state = torch.randn(batch_size, all_state_dim)
        all_action = torch.randn(batch_size, all_action_dim)
        
        print(f"\nè¾“å…¥ç»´åº¦:")
        print(f"  all_state: {all_state.shape}")
        print(f"  all_action: {all_action.shape}")
        
        # å‰å‘ä¼ æ’­
        q_value = critic(all_state, all_action)
        print(f"\nè¾“å‡ºç»´åº¦:")
        print(f"  q_value: {q_value.shape}")
        print(f"  æœŸæœ›ç»´åº¦: ({batch_size}, 1)")
        
        print(f"\nQå€¼èŒƒå›´:")
        print(f"  Qå€¼æœ€å°å€¼: {q_value.min().item():.4f}")
        print(f"  Qå€¼æœ€å¤§å€¼: {q_value.max().item():.4f}")
        print(f"  Qå€¼å¹³å‡å€¼: {q_value.mean().item():.4f}")
        
        # æ£€æŸ¥ç»´åº¦æ˜¯å¦æ­£ç¡®
        assert q_value.shape == (batch_size, 1), f"Criticè¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›{(batch_size, 1)}, å®é™…{q_value.shape}"
        
        print(f"âœ“ Criticç½‘ç»œæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ¢¯åº¦ä¼ æ’­
        print(f"\n{'='*30}")
        print("æµ‹è¯•æ¢¯åº¦ä¼ æ’­...")
        print(f"{'='*30}")
        
        # Actoræ¢¯åº¦æµ‹è¯•
        actor.zero_grad()
        action = actor(self_state, observe_length, observe_type)
        loss = action.sum()
        loss.backward()
        
        actor_grad_norm = 0
        for param in actor.parameters():
            if param.grad is not None:
                actor_grad_norm += param.grad.data.norm(2).item() ** 2
        actor_grad_norm = actor_grad_norm ** 0.5
        
        print(f"Actoræ¢¯åº¦èŒƒæ•°: {actor_grad_norm:.6f}")
        assert actor_grad_norm > 0, "Actoræ¢¯åº¦ä¸ºé›¶"
        print(f"âœ“ Actoræ¢¯åº¦ä¼ æ’­æ­£å¸¸")
        
        # Criticæ¢¯åº¦æµ‹è¯•
        critic.zero_grad()
        q_value = critic(all_state, all_action)
        loss = q_value.sum()
        loss.backward()
        
        critic_grad_norm = 0
        for param in critic.parameters():
            if param.grad is not None:
                critic_grad_norm += param.grad.data.norm(2).item() ** 2
        critic_grad_norm = critic_grad_norm ** 0.5
        
        print(f"Criticæ¢¯åº¦èŒƒæ•°: {critic_grad_norm:.6f}")
        assert critic_grad_norm > 0, "Criticæ¢¯åº¦ä¸ºé›¶"
        print(f"âœ“ Criticæ¢¯åº¦ä¼ æ’­æ­£å¸¸")
        
        # æµ‹è¯•å¤šæ™ºèƒ½ä½“åœºæ™¯
        print(f"\n{'='*30}")
        print("æµ‹è¯•å¤šæ™ºèƒ½ä½“åœºæ™¯...")
        print(f"{'='*30}")
        
        # åˆ›å»ºå¤šä¸ªActorç½‘ç»œ
        actors = []
        for i in range(num_agents):
            actors.append(Actor(self_state_dim, observe_state_dim, action_dim, hidden_dim, max_action, device))
        
        print(f"âœ“ åˆ›å»ºäº†{num_agents}ä¸ªActorç½‘ç»œ")
        
        # æµ‹è¯•æ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œé€‰æ‹©
        all_actions = []
        for i in range(num_agents):
            agent_state = torch.randn(batch_size, self_state_dim)
            agent_observe_length = torch.randn(batch_size, observe_state_dim)
            agent_observe_type = torch.randn(batch_size, observe_state_dim)
            
            agent_action = actors[i](agent_state, agent_observe_length, agent_observe_type)
            all_actions.append(agent_action)
            
            print(f"  æ™ºèƒ½ä½“{i}åŠ¨ä½œç»´åº¦: {agent_action.shape}")
        
        # æ‹¼æ¥æ‰€æœ‰åŠ¨ä½œ
        combined_actions = torch.cat(all_actions, dim=1)
        print(f"åˆå¹¶ååŠ¨ä½œç»´åº¦: {combined_actions.shape}")
        print(f"æœŸæœ›ç»´åº¦: ({batch_size}, {all_action_dim})")
        
        assert combined_actions.shape == (batch_size, all_action_dim), "å¤šæ™ºèƒ½ä½“åŠ¨ä½œæ‹¼æ¥ç»´åº¦é”™è¯¯"
        print(f"âœ“ å¤šæ™ºèƒ½ä½“åŠ¨ä½œæ‹¼æ¥æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•ä¸Criticçš„å…¼å®¹æ€§
        q_value_multi = critic(all_state, combined_actions)
        print(f"å¤šæ™ºèƒ½ä½“Criticè¾“å‡ºç»´åº¦: {q_value_multi.shape}")
        assert q_value_multi.shape == (batch_size, 1), "å¤šæ™ºèƒ½ä½“Criticè¾“å‡ºç»´åº¦é”™è¯¯"
        print(f"âœ“ å¤šæ™ºèƒ½ä½“ä¸Criticå…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
        
        print(f"\n{'='*60}")
        print("ğŸ‰ æ‰€æœ‰ç½‘ç»œæµ‹è¯•é€šè¿‡ï¼")
        print("âœ“ Actorç½‘ç»œåŠŸèƒ½æ­£å¸¸")
        print("âœ“ Criticç½‘ç»œåŠŸèƒ½æ­£å¸¸") 
        print("âœ“ æ¢¯åº¦ä¼ æ’­æ­£å¸¸")
        print("âœ“ å¤šæ™ºèƒ½ä½“åœºæ™¯å…¼å®¹")
        print("âœ“ ç»´åº¦åŒ¹é…æ­£ç¡®")
        print("âœ“ åŠ¨ä½œèŒƒå›´åˆç†")
        print(f"{'='*60}")
        
    except Exception as e:
        print(f"\nâŒ ç½‘ç»œæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e

if __name__ == "__main__":
    test_network()